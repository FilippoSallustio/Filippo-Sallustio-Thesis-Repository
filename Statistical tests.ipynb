{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Forecasting Stock Prices using Machine Learning Algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filis\\miniconda3\\envs\\thesis\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "ARIMA vs ANN\n",
      "DM statistic: -8.4035, p-value: 0.0000\n",
      "\n",
      "ARIMA vs LSTM\n",
      "DM statistic: -12.0016, p-value: 0.0000\n",
      "\n",
      "EGARCH vs XGBoost\n",
      "DM statistic: 2.3776, p-value: 0.0281\n",
      "\n",
      "GJR-GARCH vs XGBoost\n",
      "DM statistic: 2.5569, p-value: 0.0193\n"
     ]
    }
   ],
   "source": [
    "# Compare multiple forecasting models using the Diebold-Mariano test\n",
    "# This script trains ARIMA, ANN, LSTM, EGARCH, GJR-GARCH and XGBoost models\n",
    "# on the FTSE MIB dataset and performs DM tests on their one-step-ahead\n",
    "# forecasts. Models are intentionally lightweight so the script remains\n",
    "# a manageable example.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import t\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow import keras\n",
    "from arch import arch_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def dm_test(actual, pred1, pred2):\n",
    "    \"\"\"Return DM statistic and p-value for squared-error loss.\"\"\"\n",
    "    e1 = actual - pred1\n",
    "    e2 = actual - pred2\n",
    "    d = e1 ** 2 - e2 ** 2\n",
    "    T = len(d)\n",
    "    stat = d.mean() / np.sqrt(d.var(ddof=1) / T)\n",
    "    p = 2 * t.sf(np.abs(stat), df=T - 1)\n",
    "    return stat, p\n",
    "\n",
    "\n",
    "def load_data(path=\"dataftsemib_manual.csv\"):\n",
    "    \"\"\"Load and clean the FTSE MIB dataset.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "    for col in [\"Price\", \"Open\", \"High\", \"Low\"]:\n",
    "        df[col] = df[col].str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "    def parse_volume(v: str) -> float:\n",
    "        v = str(v).strip()\n",
    "        if v.endswith(\"M\"):\n",
    "            return float(v[:-1].replace(\",\", \"\")) * 1e6\n",
    "        if v.endswith(\"B\"):\n",
    "            return float(v[:-1].replace(\",\", \"\")) * 1e9\n",
    "        return float(v.replace(\",\", \"\"))\n",
    "\n",
    "    df[\"Vol.\"] = df[\"Vol.\"].apply(parse_volume).ffill()\n",
    "    df[\"Change %\"] = (\n",
    "        df[\"Change %\"].str.replace(\"%\", \"\").str.replace(\",\", \".\").astype(float)\n",
    "    )\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rolling_forecast_arima(train, test, order=(5, 1, 0)):\n",
    "    \"\"\"One-step rolling forecast with an ARIMA model.\"\"\"\n",
    "    fit = ARIMA(train, order=order).fit()\n",
    "    preds = []\n",
    "    rolling = fit\n",
    "    for obs in test:\n",
    "        pred = rolling.forecast()\n",
    "        preds.append(pred[0])\n",
    "        rolling = rolling.append([obs], refit=False)\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "def ann_forecast(series, window=60, epochs=20):\n",
    "    scaler = MinMaxScaler()\n",
    "    split = int(len(series) * 0.8)\n",
    "    scaler.fit(series[:split].reshape(-1, 1))\n",
    "    scaled = scaler.transform(series.reshape(-1, 1))\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(scaled)):\n",
    "        X.append(scaled[i - window : i, 0])\n",
    "        y.append(scaled[i, 0])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_train, X_test = X[: split - window], X[split - window :]\n",
    "    y_train, y_test = y[: split - window], y[split - window :]\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(window,)),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    )\n",
    "    pred_scaled = model.predict(X_test)\n",
    "    return scaler.inverse_transform(pred_scaled).flatten(), scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "def lstm_forecast(series, window=60, epochs=20):\n",
    "    scaler = MinMaxScaler()\n",
    "    split = int(len(series) * 0.8)\n",
    "    scaler.fit(series[:split].reshape(-1, 1))\n",
    "    scaled = scaler.transform(series.reshape(-1, 1))\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(scaled)):\n",
    "        X.append(scaled[i - window : i])\n",
    "        y.append(scaled[i, 0])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_train, X_test = X[: split - window], X[split - window :]\n",
    "    y_train, y_test = y[: split - window], y[split - window :]\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.LSTM(64, return_sequences=True, input_shape=(window, 1)),\n",
    "        keras.layers.LSTM(64),\n",
    "        keras.layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    )\n",
    "    pred_scaled = model.predict(X_test)\n",
    "    return scaler.inverse_transform(pred_scaled).flatten(), scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "def egarch_forecast(returns, exog, split_idx, order=(1, 1, 1), dist=\"t\"):\n",
    "    train_r, test_r = returns[:split_idx], returns[split_idx:]\n",
    "    exog_train, exog_test = exog[:split_idx], exog[split_idx:]\n",
    "    history_y = train_r.copy()\n",
    "    history_x = exog_train.copy()\n",
    "    ex_arrays = {c: exog_test[c].values for c in exog.columns}\n",
    "    forecasts = []\n",
    "    n_fore = min(20, len(test_r))\n",
    "    for i in range(n_fore):\n",
    "        am = arch_model(\n",
    "            history_y,\n",
    "            x=history_x,\n",
    "            mean=\"ARX\",\n",
    "            lags=1,\n",
    "            vol=\"EGARCH\",\n",
    "            p=order[0],\n",
    "            o=order[1],\n",
    "            q=order[2],\n",
    "            dist=dist,\n",
    "            rescale=False,\n",
    "        )\n",
    "        res = am.fit(disp=\"off\")\n",
    "        fc = res.forecast(horizon=1, x={c: ex_arrays[c][i : i + 1] for c in exog.columns})\n",
    "        forecasts.append(np.log(fc.variance.iloc[-1, 0]))  # if DataFrame\n",
    "        history_y = pd.concat([history_y, pd.Series([test_r.iloc[i]])], ignore_index=True)\n",
    "        history_x = pd.concat([history_x, exog_test.iloc[i : i + 1]], ignore_index=True)\n",
    "    return np.array(forecasts), returns.index[split_idx : split_idx + n_fore]\n",
    "\n",
    "\n",
    "def gjrgarch_forecast(returns, exog, split_idx, order=(1, 1, 1), dist=\"t\"):\n",
    "    train_r, test_r = returns[:split_idx], returns[split_idx:]\n",
    "    exog_train, exog_test = exog[:split_idx], exog[split_idx:]\n",
    "    history_y = train_r.copy()\n",
    "    history_x = exog_train.copy()\n",
    "    ex_arrays = {c: exog_test[c].values for c in exog.columns}\n",
    "    forecasts = []\n",
    "    n_fore = min(20, len(test_r))\n",
    "    for i in range(n_fore):\n",
    "        am = arch_model(\n",
    "            history_y,\n",
    "            x=history_x,\n",
    "            mean=\"ARX\",\n",
    "            lags=1,\n",
    "            vol=\"GARCH\",\n",
    "            p=order[0],\n",
    "            o=order[1],\n",
    "            q=order[2],\n",
    "            power=2.0,\n",
    "            dist=dist,\n",
    "            rescale=False,\n",
    "        )\n",
    "        res = am.fit(disp=\"off\")\n",
    "        fc = res.forecast(horizon=1, x={c: ex_arrays[c][i : i + 1] for c in exog.columns})\n",
    "        forecasts.append(np.log(fc.variance.values[-1, 0]))\n",
    "        history_y = pd.concat([history_y, pd.Series([test_r.iloc[i]])], ignore_index=True)\n",
    "        history_x = pd.concat([history_x, exog_test.iloc[i : i + 1]], ignore_index=True)\n",
    "    return np.array(forecasts), returns.index[split_idx : split_idx + n_fore]\n",
    "\n",
    "\n",
    "def xgb_forecast(features, target, split_idx):\n",
    "    train_X, test_X = features.iloc[:split_idx], features.iloc[split_idx:]\n",
    "    train_y, test_y = target.iloc[:split_idx], target.iloc[split_idx:]\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    pred = model.predict(test_X)\n",
    "    return pred, test_y\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Load data and prepare common series\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "df = load_data()\n",
    "prices = df[\"Price\"].values\n",
    "split = int(len(prices) * 0.8)\n",
    "\n",
    "# Returns and volatility features for GARCH and XGBoost\n",
    "returns = np.log(df[\"Price\"]).diff() * 100\n",
    "log_var = np.log(returns.pow(2) + 1e-8)\n",
    "log_vol = np.log(df[\"Vol.\"])\n",
    "price_range = (np.log(df[\"High\"]) - np.log(df[\"Low\"])) * 100\n",
    "realized = 0.5 * returns.pow(2) + 0.5 * price_range.pow(2)\n",
    "exog = pd.DataFrame({\n",
    "    \"LogVol\": log_vol,\n",
    "    \"RV_lag1\": realized.shift(1),\n",
    "    \"ShockLag1\": returns.abs().shift(1),\n",
    "})\n",
    "features_xgb = pd.DataFrame({\n",
    "    \"logvol_lag1\": log_vol.shift(1),\n",
    "    \"ret_lag1\": returns.shift(1),\n",
    "    \"rv_lag1\": realized.shift(1),\n",
    "    \"range_lag1\": price_range.shift(1),\n",
    "})\n",
    "\n",
    "df_feat = pd.concat([log_var.shift(-1).rename(\"log_var\"), features_xgb], axis=1).dropna()\n",
    "features_xgb = df_feat.drop(columns=[\"log_var\"])\n",
    "target_xgb = df_feat[\"log_var\"]\n",
    "\n",
    "# Drop NaNs created by differencing\n",
    "returns = returns.dropna()\n",
    "log_var = log_var.loc[returns.index]\n",
    "exog = exog.loc[returns.index].dropna()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Model forecasts\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# ARIMA forecast of closing prices\n",
    "arima_pred = rolling_forecast_arima(prices[:split], prices[split:])\n",
    "prices_test = prices[split:][: len(arima_pred)]\n",
    "\n",
    "# ANN forecast\n",
    "ann_pred, ann_true = ann_forecast(prices, window=60, epochs=10)\n",
    "ann_pred = ann_pred[-len(arima_pred):]\n",
    "ann_true = ann_true[-len(arima_pred):]\n",
    "\n",
    "# LSTM forecast\n",
    "lstm_pred, lstm_true = lstm_forecast(prices, window=60, epochs=10)\n",
    "lstm_pred = lstm_pred[-len(arima_pred):]\n",
    "lstm_true = lstm_true[-len(arima_pred):]\n",
    "\n",
    "# EGARCH forecast of log variance\n",
    "egarch_pred, egarch_idx = egarch_forecast(returns, exog, split)\n",
    "true_lv = log_var.iloc[split:][: len(egarch_pred)]\n",
    "\n",
    "# GJR-GARCH forecast of log variance\n",
    "gjr_pred, gjr_idx = gjrgarch_forecast(returns, exog, split)\n",
    "true_lv_gjr = log_var.iloc[split:][: len(gjr_pred)]\n",
    "\n",
    "# XGBoost forecast of log variance\n",
    "xgb_pred, xgb_true = xgb_forecast(features_xgb, target_xgb, split)\n",
    "xgb_pred = xgb_pred[: len(true_lv)]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Diebold-Mariano tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"ARIMA vs ANN\")\n",
    "stat, p = dm_test(prices_test, arima_pred, ann_pred)\n",
    "print(f\"DM statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"ARIMA vs LSTM\")\n",
    "stat, p = dm_test(prices_test, arima_pred, lstm_pred)\n",
    "print(f\"DM statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"EGARCH vs XGBoost\")\n",
    "stat, p = dm_test(true_lv.values, egarch_pred, xgb_pred)\n",
    "print(f\"DM statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"GJR-GARCH vs XGBoost\")\n",
    "stat, p = dm_test(true_lv_gjr.values, gjr_pred, xgb_pred[: len(gjr_pred)])\n",
    "print(f\"DM statistic: {stat:.4f}, p-value: {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
